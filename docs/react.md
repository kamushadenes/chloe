# ReAct

ReAct comes from the
paper [ReAct: Synergizing Reasoning and Acting in Language Models](https://react-lm.github.io) and
is a way to improve the quality of prompts generated by Chloe (or any LLM for that matter).

![ReAct](https://github.com/kamushadenes/chloe/raw/main/.github/resources/images/react.png)

Below is the abstract from the paper:

```
While large language models (LLMs) have demonstrated impressive capabilities across tasks in
language understanding and interactive decision making, their abilities for reasoning (e.g.
chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as
separate topics. 

In this paper, we explore the use of LLMs to generate both reasoning traces and
task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions,
while actions allow it to interface with external sources, such as knowledge bases or environments,
to gather additional information. 

We apply our approach, named ReAct, to a diverse set of language
and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well
as improved human interpretability and trustworthiness over methods without reasoning or acting
components. 

Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct
overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by
interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are
more interpretable than baselines without reasoning traces. 

On two interactive decision making
benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by
an absolute success rate of 34% and 10% respectively, while being prompted with only one or two
in-context examples.
```

This package also includes all the actions, such as `google`, `wikipedia`, `news`, etc. that Chloe
supports. The documentation for these actions can be found in the [actions](https://github.com/kamushadenes/chloe/blob/main/docs/actions.md) page.